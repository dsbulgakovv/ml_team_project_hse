{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers huggingface_hub sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKsKiLlEBEw8",
        "outputId": "aa2a06d8-3cf8-430a-a065-e671dd29320b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "\n",
        "REPO_ID = \"MonoHime/ru_sentiment_dataset\"\n",
        "FILENAME = \"datasets.csv\"\n",
        "\n",
        "dataset = pd.read_csv(\n",
        "    hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=\"dataset\"), index_col=0\n",
        ")"
      ],
      "metadata": {
        "id": "924rc-KEhSBs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1312235-288c-4da3-caa0-7587f4f29144"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_ = 'texts.txt'\n",
        "with open(file_, 'a') as f:\n",
        "  for text in dataset.text:\n",
        "    f.write(text)"
      ],
      "metadata": {
        "id": "NeRM73MP_Gkj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(dataset, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "fRugLebhhTN9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentencepiece import SentencePieceTrainer, SentencePieceProcessor"
      ],
      "metadata": {
        "id": "RqFaNQkd91q6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from typing import Union, List, Tuple\n",
        "from sentencepiece import SentencePieceTrainer, SentencePieceProcessor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    TRAIN_VAL_RANDOM_SEED = 42\n",
        "    VAL_RATIO = 0.05\n",
        "\n",
        "    def __init__(self, data_file: str, dataset, train: bool = True, sp_model_prefix: str = None,\n",
        "                 vocab_size: int = 5000, normalization_rule_name: str = 'nmt_nfkc_cf',\n",
        "                 model_type: str = 'bpe', max_length: int = 256):\n",
        "\n",
        "        if not os.path.isfile(sp_model_prefix + '.model'):\n",
        "            SentencePieceTrainer.train(\n",
        "                input=data_file, vocab_size=vocab_size,\n",
        "                model_type=model_type, model_prefix=sp_model_prefix,\n",
        "                normalization_rule_name=normalization_rule_name,\n",
        "                unk_id=0, bos_id=1, eos_id=2, pad_id=3\n",
        "            )\n",
        "        self.sp_model = SentencePieceProcessor(model_file=sp_model_prefix + '.model')\n",
        "\n",
        "        with open(data_file) as file:\n",
        "            texts = file.readlines()\n",
        "\n",
        "        random.seed(self.TRAIN_VAL_RANDOM_SEED)\n",
        "        random.shuffle(texts)\n",
        "        df_train, df_val = train_test_split(dataset, test_size=self.VAL_RATIO, random_state=self.TRAIN_VAL_RANDOM_SEED)\n",
        "\n",
        "        self.df = df_train if train else df_val\n",
        "        self.indices = self.sp_model.encode(self.df.text.tolist())\n",
        "\n",
        "        self.pad_id, self.unk_id, self.bos_id, self.eos_id = \\\n",
        "            self.sp_model.pad_id(), self.sp_model.unk_id(), \\\n",
        "            self.sp_model.bos_id(), self.sp_model.eos_id()\n",
        "        self.max_length = max_length\n",
        "        self.vocab_size = self.sp_model.vocab_size()\n",
        "\n",
        "    def text2ids(self, texts: Union[str, List[str]]) -> Union[List[int], List[List[int]]]:\n",
        "        return self.sp_model.encode(texts)\n",
        "\n",
        "    def ids2text(self, ids: Union[torch.Tensor, List[int], List[List[int]]]) -> Union[str, List[str]]:\n",
        "\n",
        "        return self.sp_model.decode(ids)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, item: int) -> Tuple[torch.Tensor, int]:\n",
        "        indices = torch.tensor([self.bos_id, *self.indices[item], self.eos_id])\n",
        "        if len(indices) < self.max_length:\n",
        "            padding = torch.full((self.max_length - len(indices),), self.pad_id)\n",
        "            indices = torch.cat((indices, padding))\n",
        "        else:\n",
        "            indices = indices[:self.max_length-1]\n",
        "            indices = torch.cat((indices, torch.tensor([self.eos_id])))\n",
        "        return indices, self.df.sentiment.iloc[item]\n"
      ],
      "metadata": {
        "id": "0qHzgSlUhTQo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 5000\n",
        "MAX_LENGTH = 256\n",
        "\n",
        "train_set = TextDataset(data_file='texts.txt', dataset=dataset, vocab_size=VOCAB_SIZE, train=True, sp_model_prefix='bpe', max_length=MAX_LENGTH)\n",
        "valid_set = TextDataset(data_file='texts.txt', dataset=dataset, vocab_size=VOCAB_SIZE, train=False, sp_model_prefix='bpe', max_length=MAX_LENGTH)"
      ],
      "metadata": {
        "id": "-I0Plk3ChTXn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "hFkP2-5tFXxp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(hidden_dim, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        output = self.dropout(output)\n",
        "        last_hidden = output[:, -1, :]\n",
        "        logits = self.fc(last_hidden)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "7YsAfHREhTce"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "PwxJsTvWTXVF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, criterion, optimizer, train_loader, val_loader, epoch, scheduler=None):\n",
        "  for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      total_loss = 0.0\n",
        "      total_samples = 0\n",
        "      train_acc = []\n",
        "\n",
        "\n",
        "      for inputs, targets in tqdm(train_loader):\n",
        "          inputs = inputs.to(device)\n",
        "          targets = targets.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs.view(-1, 3), targets.view(-1))\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          total_loss += loss.item() * len(inputs)\n",
        "          acc = (targets.view(-1) == outputs.view(-1, 3).argmax(-1)).sum() / len(inputs)\n",
        "          train_acc.append(acc.cpu())\n",
        "          total_samples += len(inputs)\n",
        "          if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "      model.eval()\n",
        "      total_val_loss = 0.0\n",
        "      total_val_samples = 0\n",
        "      val_acc = []\n",
        "      with torch.no_grad():\n",
        "          for inputs, targets in valid_loader:\n",
        "              inputs = inputs.to(device)\n",
        "              targets = targets.to(device)\n",
        "              outputs = model(inputs)\n",
        "              val_loss = criterion(outputs.view(-1, 3), targets.view(-1))\n",
        "\n",
        "              total_val_loss += val_loss.item() * len(inputs)\n",
        "              acc = (targets.view(-1) == outputs.view(-1, 3).argmax(-1)).sum() / len(inputs)\n",
        "              val_acc.append(acc.cpu())\n",
        "              total_val_samples += len(inputs)\n",
        "\n",
        "      avg_loss = total_loss / total_samples\n",
        "      train_acc = np.mean(train_acc)\n",
        "      val_acc = np.mean(val_acc)\n",
        "      avg_val_loss = total_val_loss / total_val_samples\n",
        "\n",
        "      print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "  return model"
      ],
      "metadata": {
        "id": "vlnitJmhavG1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = train_set.vocab_size\n",
        "embedding_dim = 256\n",
        "hidden_dim = 128\n",
        "batch_size = 32\n",
        "num_epochs = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "model = TextClassifier(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=4, eta_min=0)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "\n",
        "model = train(model, criterion, optimizer, train_loader, valid_loader, num_epochs, scheduler)"
      ],
      "metadata": {
        "id": "i5iSwEvNhTe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f2470d7-dc3b-42b7-dad5-744d284e18ef"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6264/6264 [00:52<00:00, 118.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Train Loss: 1.0009, Val Loss: 0.7802, Train Acc: 0.4915, Val Acc: 0.6109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6264/6264 [00:52<00:00, 118.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Train Loss: 0.6644, Val Loss: 0.5819, Train Acc: 0.6783, Val Acc: 0.7200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6264/6264 [00:54<00:00, 114.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Train Loss: 0.5516, Val Loss: 0.5295, Train Acc: 0.7432, Val Acc: 0.7581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6264/6264 [00:53<00:00, 117.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Train Loss: 0.4995, Val Loss: 0.5144, Train Acc: 0.7715, Val Acc: 0.7656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6264/6264 [00:53<00:00, 116.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Train Loss: 0.4602, Val Loss: 0.5221, Train Acc: 0.7936, Val Acc: 0.7679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model.pth')"
      ],
      "metadata": {
        "id": "ymgD3mtWF3h6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"yqelz/xml-roberta-large-ner-russian\")\n",
        "ner_model = AutoModelForTokenClassification.from_pretrained(\"yqelz/xml-roberta-large-ner-russian\")"
      ],
      "metadata": {
        "id": "qFyWomc3geBu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_pipeline = pipeline(\"ner\", model=ner_model, tokenizer=tokenizer)\n",
        "sp_model = SentencePieceProcessor(model_file='bpe.model')\n",
        "model = TextClassifier(vocab_size, embedding_dim, hidden_dim)\n",
        "model.load_state_dict(torch.load('model.pth'), strict=False)"
      ],
      "metadata": {
        "id": "zLHXYbc4hcoQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_by_title(text, ner_pipeline):\n",
        "  sentences = text.split('.')\n",
        "  result = []\n",
        "  temp_text = ''\n",
        "  for i in range(len(sentences)):\n",
        "    text = sentences.pop(0)\n",
        "    if ner_pipeline(text):\n",
        "      if not result:\n",
        "        result.append(temp_text)\n",
        "      else:\n",
        "        result[-1] = result[-1] + temp_text\n",
        "      temp_text = text\n",
        "    else:\n",
        "      temp_text = '.'.join([temp_text, text])\n",
        "  else:\n",
        "    result.append(temp_text)\n",
        "  return result\n",
        "\n",
        "\n",
        "def get_sentiments(classification_model, classification_tokenizer, ner_pipeline, text, device):\n",
        "  mapping = {\n",
        "      0: 'neutral',\n",
        "      1: 'positive',\n",
        "      2: 'negative'\n",
        "  }\n",
        "  result = []\n",
        "  classification_model = classification_model.to(device)\n",
        "  texts = split_by_title(text, ner_pipeline)\n",
        "  model.eval()\n",
        "  for text in texts:\n",
        "    tokens = torch.tensor(classification_tokenizer.encode(text)).unsqueeze(0).to(device)\n",
        "    logits = model(tokens).cpu()\n",
        "    ner_res = ner_pipeline(text)\n",
        "    if ner_res:\n",
        "      movie_title = ''\n",
        "      end = 0\n",
        "      for entity in ner_res:\n",
        "        if entity.get('entity') == 'B-ORG':\n",
        "          movie_title += entity.get('word', '').replace('▁', ' ')\n",
        "          end = entity.get('end')\n",
        "        if (entity.get('entity') == 'I-ORG'):\n",
        "          movie_title += entity.get('word', '').replace('▁', ' ')\n",
        "          end = entity.get('end')\n",
        "    result.append({\n",
        "          'title': movie_title.lstrip(),\n",
        "          'setiment': mapping[logits.argmax(-1).numpy()[0]]\n",
        "          })\n",
        "  return result"
      ],
      "metadata": {
        "id": "RJDerSRimPD7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Во все тяжкие это лучший сериал из всех что я смотрел. А вот Игра престолов оказалась абсолютным бредом'\n",
        "result = get_sentiments(model, sp_model, ner_pipeline, text, device)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFVhW_SQiK0x",
        "outputId": "6a411484-a354-4be7-cb96-ad148d419d6d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'Во все тяжкие', 'setiment': 'positive'},\n",
              " {'title': 'Игра престолов', 'setiment': 'negative'}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KP_ouOVyoBwv"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}
